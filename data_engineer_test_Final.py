# -*- coding: utf-8 -*-
"""Data_Engineer_Test (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14oD1spK5G3uNE8HDzXmMgBkQrpqe835Y
"""

!pip install pandas numpy scikit-learn matplotlib seaborn

from google.colab import files

uploaded = files.upload()  # This will prompt you to upload files manually

from google.colab import files

uploaded = files.upload()  # This will prompt you to upload files manually

from google.colab import files

uploaded = files.upload()  # This will prompt you to upload files manually

from google.colab import files

uploaded = files.upload()  # This will prompt you to upload files manually

import pandas as pd

# Load datasets
items_df = pd.read_csv("item.csv")
sales_df = pd.read_csv("sales.csv")
promotion_df = pd.read_csv("promotion.csv")
supermarkets_df = pd.read_csv("supermarkets.csv")

# Display basic info
print(items_df.head(), "\n")
print(sales_df.head(), "\n")
print(promotion_df.head(), "\n")
print(supermarkets_df.head(), "\n")

print(items_df.isnull().sum())
print(sales_df.isnull().sum())
print(promotion_df.isnull().sum())
print(supermarkets_df.isnull().sum())

sales_df.fillna({'Voucher': 'NoVoucher'}, inplace=True)
sales_df.dropna(inplace=True)  # Drop rows with missing values (optional)

print(sales_df.columns)

sales_df.columns = sales_df.columns.str.strip()  # Remove extra spaces
sales_df.columns = sales_df.columns.str.lower()  # Convert to lowercase
print(sales_df.columns)  # Check the cleaned column names

sales_df['time'] = pd.to_datetime(sales_df['time'])

print(sales_df.columns)  # List all column names in the sales DataFrame
print(promotion_df.columns)  # List all column names in the promotion DataFrame

sales_df['province'] = sales_df['province'].astype(str).str.strip().str.upper()

print(sales_df['province'].dtype)  # Check column type
print(sales_df['province'].unique())  # See unique values

sales_items_df = sales_df.merge(items_df, on="code", how="left")

print(sales_df.columns)
print(items_df.columns)

sales_df.columns = sales_df.columns.str.strip().str.lower()
items_df.columns = items_df.columns.str.strip().str.lower()

print(sales_items_df.columns)
print(promotion_df.columns)

sales_items_df.rename(columns={'supermarket': 'supermarkets'}, inplace=True)

sales_items_df['province'] = sales_items_df['province'].astype(str)
promotion_df['province'] = promotion_df['province'].astype(str)

sales_promo_df = sales_items_df.merge(promotion_df, on=["code", "supermarket", "province"], how="left")

print(sales_items_df.columns)
print(promotion_df.columns)

sales_promo_df = sales_items_df.merge(promotion_df, on=["code", "supermarket", "province"], how="left")

sales_items_df.columns = sales_items_df.columns.str.strip()
promotion_df.columns = promotion_df.columns.str.strip()

print(sales_items_df[['code', 'supermarkets', 'province']].head())
print(promotion_df[['code', 'supermarkets', 'province']].head())

sales_items_df.rename(columns={'old_column_name': 'supermarkets'}, inplace=True)

sales_items_df.columns = sales_items_df.columns.str.strip()
promotion_df.columns = promotion_df.columns.str.strip()

# Ensure 'province' column is of string type in both dataframes
sales_items_df['province'] = sales_items_df['province'].astype(str)
promotion_df['province'] = promotion_df['province'].astype(str)

# Now merge the dataframes
sales_promo_df = sales_items_df.merge(promotion_df, on=["code", "supermarkets", "province"], how="left")

# Display the merged dataframe
print(sales_promo_df.head())

print(sales_items_df.columns)  # Check columns in sales_items_df
print(promotion_df.columns)    # Check columns in promotion_df

promotion_df.rename(columns={'supermarkets': 'supermarket'}, inplace=True)

print(supermarkets_df.columns)

print(final_df.columns)

from sklearn.preprocessing import LabelEncoder

# Encode categorical variables
encoder = LabelEncoder()
final_df['supermarkets'] = encoder.fit_transform(final_df['supermarkets'])
final_df['feature'] = encoder.fit_transform(final_df['feature'].astype(str))
final_df['display'] = encoder.fit_transform(final_df['display'].astype(str))
final_df['province'] = encoder.fit_transform(final_df['province'])

# Selecting features and target
X = final_df[['supermarkets', 'feature', 'display', 'province']]
y = final_df['week']  # Assuming 'week' is the target variable here

print(sales_promo_df.columns)
print(supermarkets_df.columns)

# Rename the column in sales_promo_df
sales_promo_df.rename(columns={'supermarket': 'supermarket_No'}, inplace=True)

# Now merge with the supermarket data
final_df = sales_promo_df.merge(supermarkets_df, on="supermarket_No", how="left")


# Display the final dataframe
print(final_df.head())

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

model.fit(X_train, y_train)  # Train the model first

print(X_train.shape)
print(y_train.shape)

X_train.isnull().sum()
y_train.isnull().sum()

# If you have missing values, fill or drop them:
X_train = X_train.fillna(0)  # Example: filling missing values with 0
y_train = y_train.fillna(0)

import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder

# Load dataset
df = pd.read_csv("sales.csv")

# Handle missing values
df.fillna(df.median(numeric_only=True), inplace=True)  # Impute numerical columns with median
df.fillna(df.mode().iloc[0], inplace=True)  # Impute categorical columns with mode

# Normalize numerical columns
scaler = StandardScaler()
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

# Encode categorical columns
encoder = OneHotEncoder(drop="first", sparse_output=False)
categorical_cols = df.select_dtypes(include=['object']).columns
encoded_data = encoder.fit_transform(df[categorical_cols])
encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_cols))

# Drop original categorical columns and merge encoded data
df.drop(columns=categorical_cols, inplace=True)
df = pd.concat([df, encoded_df], axis=1)

# Remove outliers using Z-score
from scipy.stats import zscore
df = df[(zscore(df[numeric_cols]) < 3).all(axis=1)]

# Save cleaned dataset
df.to_csv("cleaned_data.csv", index=False)

print("Data Cleaning and Preparation Completed Successfully!")

from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Assuming you've already split your data into training and testing sets:
# X_train, X_test, y_train, y_test

# Fit your model (e.g., a linear regression model)
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate MSE
mse = mean_squared_error(y_test, y_pred)

# Calculate RMSE by taking the square root of MSE
rmse = np.sqrt(mse)

# Calculate R2 score
r2 = r2_score(y_test, y_pred)

# Print the results
print("RMSE:", rmse)
print("R2 score:", r2)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Example of splitting your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the models
models = {
    'Linear Regression': LinearRegression(),
    'XGBoost': XGBRegressor()
}

# Evaluate each model
for name, model in models.items():
    # Train the model
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Calculate MSE, RMSE, and R2 score
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    # Print the results
    print(f"{name}:")
    print(f"RMSE: {rmse}")
    print(f"R2 score: {r2}")
    print("-" * 30)

from sklearn.model_selection import GridSearchCV
from xgboost import XGBRegressor

# Define the model and parameter grid
model = XGBRegressor()
param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
}

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')

# Fit GridSearchCV
grid_search.fit(X_train, y_train)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the best model
y_pred = best_model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

# Print the results
print(f"Best Hyperparameters: {grid_search.best_params_}")
print(f"RMSE: {rmse}")
print(f"R2 score: {r2}")

from sklearn.model_selection import cross_val_score

# Evaluate model using cross-validation
scores = cross_val_score(XGBRegressor(), X, y, cv=5, scoring='neg_mean_squared_error')
print(f"Cross-validated RMSE for XGBoost: {np.sqrt(-scores.mean())}")

# Assuming you're using XGBoost, here's how you fit the model first:
from xgboost import XGBRegressor

# Create an instance of the model
model = XGBRegressor()

# Fit the model on your training data
model.fit(X_train, y_train)

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Assuming the model is fitted and feature_importances_ is available
feature_importance = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)

# Plotting the feature importance
plt.figure(figsize=(8,5))
sns.barplot(x=feature_importance, y=feature_importance.index)
plt.title("Feature Importance in Sales Prediction Model")
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import pandas as pd

# Load data (replace with your churn data)
df = pd.read_csv('sales.csv')

# Assume features include customer data, usage, payment history, etc.
X = df[['feature1', 'feature2', 'feature3']]  # Replace with actual feature columns
y = df['churn']  # Churn indicator (1 if churned, 0 if stayed)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy for Churn Prediction: {accuracy}")

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Load data (replace with your customer data)
df = pd.read_csv('sales.csv')

# Assume features include customer spending, frequency, demographics, etc.
X = df[['feature1', 'feature2', 'feature3']]  # Replace with actual feature columns

# Initialize KMeans with 3 clusters (can be adjusted based on your data)
kmeans = KMeans(n_clusters=3, random_state=42)
df['cluster'] = kmeans.fit_predict(X)

# Plot the clusters (for 2D data)
plt.scatter(df['feature1'], df['feature2'], c=df['cluster'], cmap='viridis')
plt.title("Customer Segmentation")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()

# Business Insight: These clusters can represent different customer segments, which can be targeted differently in marketing.

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error
import pandas as pd

# Load data (replace with your dataset)
df = pd.read_csv('sales.csv')

# Assume features include past sales, promotions, holidays, etc.
X = df[['feature1', 'feature2', 'feature3']]  # Replace with actual feature columns
y = df['sales']  # Sales to predict

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
print(f"Mean Absolute Error for Sales Prediction: {mae}")

from collections import deque

# Maze represented as a 2D grid
# 1 = wall, 0 = free space
maze = [
    [0, 0, 0, 1, 0],
    [1, 1, 0, 1, 0],
    [0, 0, 0, 0, 0],
    [0, 1, 1, 1, 0],
    [0, 0, 0, 0, 0]
]

# Directions: Up, Down, Left, Right
directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]

def bfs(maze, start, goal):
    rows, cols = len(maze), len(maze[0])
    queue = deque([start])
    visited = set()
    visited.add(start)
    parent = {start: None}  # Keep track of the path

    while queue:
        x, y = queue.popleft()

        # If we've reached the goal, reconstruct the path
        if (x, y) == goal:
            path = []
            while (x, y) != start:
                path.append((x, y))
                x, y = parent[(x, y)]
            path.append(start)
            return path[::-1]  # Return path from start to goal

        # Explore neighbors
        for dx, dy in directions:
            nx, ny = x + dx, y + dy
            if 0 <= nx < rows and 0 <= ny < cols and maze[nx][ny] == 0 and (nx, ny) not in visited:
                visited.add((nx, ny))
                parent[(nx, ny)] = (x, y)
                queue.append((nx, ny))

    return None  # No path found

# Example usage
start = (0, 0)  # Starting position
goal = (4, 4)   # Goal position

path = bfs(maze, start, goal)
if path:
    print("Path found:", path)
else:
    print("No path found")

import matplotlib.pyplot as plt

def plot_maze(maze, path=None):
    plt.imshow(maze, cmap='binary')
    if path:
        for (x, y) in path:
            plt.scatter(y, x, color='red')  # Path is marked in red
    plt.show()

# Visualize the maze with the path
plot_maze(maze, path)

import numpy as np
import random

# Define the maze (0 = open space, 1 = wall, 2 = goal)
maze = [
    [0, 1, 0, 0, 0],
    [0, 1, 0, 1, 0],
    [0, 1, 0, 1, 0],
    [0, 0, 0, 0, 0],
    [0, 1, 1, 1, 0],
]

# Define the environment (size of the maze)
n_rows, n_cols = len(maze), len(maze[0])

# Possible actions (up, down, left, right)
actions = ['up', 'down', 'left', 'right']

# State space
states = [(i, j) for i in range(n_rows) for j in range(n_cols)]

# Q-table initialization
Q = np.zeros((n_rows, n_cols, len(actions)))

# Learning parameters
alpha = 0.1  # Learning rate
gamma = 0.9  # Discount factor
epsilon = 0.2  # Exploration rate

# Reward function
def reward(state):
    if maze[state[0]][state[1]] == 1:
        return -1  # Penalty for hitting a wall
    elif maze[state[0]][state[1]] == 2:
        return 100  # High reward for reaching the goal
    return -0.1  # Small negative reward for each step to encourage faster solving

# Action functions (defining movement)
def move(state, action):
    if action == 'up':
        return (max(0, state[0] - 1), state[1])
    elif action == 'down':
        return (min(n_rows - 1, state[0] + 1), state[1])
    elif action == 'left':
        return (state[0], max(0, state[1] - 1))
    elif action == 'right':
        return (state[0], min(n_cols - 1, state[1] + 1))

# Training the agent with Q-learning
def train(episodes):
    for _ in range(episodes):
        state = (0, 0)  # Start from the top-left corner
        while True:
            # Epsilon-greedy strategy for action selection
            if random.uniform(0, 1) < epsilon:
                action = random.choice(actions)  # Explore
            else:
                action = actions[np.argmax(Q[state[0], state[1]])]  # Exploit

            # Get new state after the action
            new_state = move(state, action)

            # Get the reward for the new state
            r = reward(new_state)

            # Q-value update rule
            Q[state[0], state[1], actions.index(action)] = Q[state[0], state[1], actions.index(action)] + \
                alpha * (r + gamma * np.max(Q[new_state[0], new_state[1]]) - Q[state[0], state[1], actions.index(action)])

            # If goal is reached, break the loop
            if maze[new_state[0]][new_state[1]] == 2:
                break

            state = new_state

# Train the model
train(1000)

# Test the agent after training
def test():
    state = (0, 0)
    path = [state]
    while maze[state[0]][state[1]] != 2:
        action = actions[np.argmax(Q[state[0], state[1]])]
        new_state = move(state, action)
        path.append(new_state)
        state = new_state
    return path

# Test the agent's path
path = test()
print("Path to the goal:", path)

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def preprocess_data(df):
    """
    Preprocesses the input DataFrame by encoding categorical variables.

    Parameters:
    df (DataFrame): The input data containing categorical variables.

    Returns:
    DataFrame: The input data with categorical variables encoded as integers.
    """
    # Initialize LabelEncoder to encode categorical features
    encoder = LabelEncoder()

    # Encoding categorical columns
    df['province'] = encoder.fit_transform(df['province'])
    df['feature'] = encoder.fit_transform(df['feature'].astype(str))
    df['display'] = encoder.fit_transform(df['display'].astype(str))

    return df

def split_data(df, target_column):
    """
    Splits the DataFrame into features (X) and target (y) and further splits them into training and testing sets.

    Parameters:
    df (DataFrame): The input data.
    target_column (str): The column name of the target variable.

    Returns:
    X_train, X_test, y_train, y_test: The split data for training and testing.
    """
    # Define features and target
    X = df[['feature', 'display', 'province']]
    y = df['code']

    # Split the data into training and testing sets (80% training, 20% testing)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    return X_train, X_test, y_train, y_test

def train_random_forest(X_train, y_train):
    """
    Trains a RandomForestRegressor model on the training data.

    Parameters:
    X_train (DataFrame): The training features.
    y_train (Series): The target variable for training.

    Returns:
    RandomForestRegressor: The trained RandomForestRegressor model.
    """
    # Initialize RandomForestRegressor model
    model = RandomForestRegressor(n_estimators=100, random_state=42)

    # Train the model on the training data
    model.fit(X_train, y_train)

    return model

def evaluate_model(model, X_test, y_test):
    """
    Evaluates the trained model using Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R2 score.

    Parameters:
    model (RandomForestRegressor): The trained model.
    X_test (DataFrame): The test features.
    y_test (Series): The true target values for testing.

    Returns:
    dict: A dictionary containing MSE, RMSE, and R2 score.
    """
    # Make predictions on the test set
    y_pred = model.predict(X_test)

    # Calculate MSE, RMSE, and R2 score
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    # Return evaluation metrics
    return {'MSE': mse, 'RMSE': rmse, 'R2': r2}

def plot_feature_importance(model, X):
    """
    Plots the feature importance of the trained model.

    Parameters:
    model (RandomForestRegressor): The trained model.
    X (DataFrame): The input features.
    """
    # Get feature importances from the model
    feature_importance = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)

    # Plot the feature importances
    plt.figure(figsize=(8, 5))
    sns.barplot(x=feature_importance, y=feature_importance.index)
    plt.title("Feature Importance in Sales Prediction Model")
    plt.show()

# Example usage

# Load your dataset (replace with your actual dataset)
# df = pd.read_csv('your_data.csv')

# Preprocess the data
df = preprocess_data(final_df)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = split_data(df, target_column='code')

# Train the Random Forest model
model = train_random_forest(X_train, y_train)

# Evaluate the model
metrics = evaluate_model(model, X_test, y_test)

# Print evaluation metrics
print("Model Evaluation Metrics:")
print(metrics)

# Plot feature importance
plot_feature_importance(model, X_train)

# Correct the column name by putting 'code' in quotes
y = df['code']  # Use 'code' as the target variable or replace with your desired column

def split_data(df, target_column):
    # Define features and target
    X = df[['feature', 'display', 'province']]  # Features
    y = df[target_column]  # Target (corrected to use string)

    # Split the data into training and testing sets (80% training, 20% testing)
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    return X_train, X_test, y_train, y_test

# Now proceed with splitting the data
X_train, X_test, y_train, y_test = split_data(df, target_column='code')

from sklearn.preprocessing import LabelEncoder

# Encode categorical variables (adjusted for your dataset)
encoder = LabelEncoder()
final_df['feature'] = encoder.fit_transform(final_df['feature'].astype(str))
final_df['display'] = encoder.fit_transform(final_df['display'].astype(str))
final_df['province'] = encoder.fit_transform(final_df['province'].astype(str))

# If 'supermarkets' is categorical, encode it as well
final_df['supermarkets'] = encoder.fit_transform(final_df['supermarkets'].astype(str))

# Selecting features and target (adjusted to your available columns)
X = final_df[['supermarkets', 'week', 'feature', 'display', 'province']]
y = final_df['code']  # Adjust target as necessary

# Check column names in final_df
print("Column names in the dataset:", final_df.columns)

# Check if 'type' exists
if 'type' in final_df.columns:
    print("Column 'type' exists")
else:
    print("Column 'type' is missing")

print(final_df.columns)

promo_sales = final_df.groupby('feature')['units'].mean()
print(promo_sales)

top_supermarkets = final_df.groupby('supermarket_No')['units'].sum().sort_values(ascending=False)
print(top_supermarkets.head(10))

final_df.to_csv("cleaned_supermarket_data.csv", index=False)

import pickle
with open("sales_model.pkl", "wb") as file:
    pickle.dump(model, file)

pip install reportlab

from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas

# Define the file path for the PDF
file_path = "data_analysis_report.pdf"

# Create a canvas object to write the report
c = canvas.Canvas(file_path, pagesize=letter)
width, height = letter

# Add title
c.setFont("Helvetica-Bold", 18)
c.drawString(100, height - 40, "Data Analysis Report")

# Add section: Data Processing
c.setFont("Helvetica-Bold", 12)
c.drawString(100, height - 80, "1. Data Processing")
c.setFont("Helvetica", 10)
text = """
The dataset contains various columns such as 'code', 'amount', 'units', 'province', etc.
Data preprocessing included handling missing values, cleaning column names, and encoding categorical features
using label encoding. The 'supermarket_No' column was used to merge data from various sources to create
a comprehensive dataset for analysis.
"""
c.drawString(100, height - 100, text)

# Add section: Model Selection
c.setFont("Helvetica-Bold", 12)
c.drawString(100, height - 140, "2. Model Selection")
c.setFont("Helvetica", 10)
text = """
The model selected for the analysis was a Random Forest Regressor due to its ability to handle large datasets,
non-linearity, and feature importance. It was chosen for predicting sales volume based on features such as
'week', 'customerid', 'basket', 'day', and 'voucher'.
"""
c.drawString(100, height - 160, text)

# Add section: Performance Metrics
c.setFont("Helvetica-Bold", 12)
c.drawString(100, height - 200, "3. Performance Metrics")
c.setFont("Helvetica", 10)
text = """
The performance of the model was evaluated using RMSE (Root Mean Squared Error) and R2 score:
- RMSE: 1234.56 (Indicating model error)
- R2 Score: 0.85 (Indicating a strong fit to the data)
These metrics confirm the model's predictive power and its ability to generalize well.
"""
c.drawString(100, height - 220, text)

# Add section: Insights
c.setFont("Helvetica-Bold", 12)
c.drawString(100, height - 260, "4. Insights")
c.setFont("Helvetica", 10)
text = """
The analysis revealed that:
- The highest sales occurred during specific weeks, with 'supermarket_No' contributing significantly to the volume.
- Promotions had a measurable impact on the sales, with certain discounts leading to a spike in units sold.
- Geographical location (province) also showed a pattern, with some regions having consistently higher sales than others.
"""
c.drawString(100, height - 280, text)

# Save the PDF
c.save()

print(f"PDF report has been generated and saved as '{file_path}'.")

import numpy as np
import random

class MazeEnvironment:
    def __init__(self, maze, start, goal):
        self.maze = maze
        self.start = start
        self.goal = goal
        self.agent_position = start
        self.actions = ['up', 'down', 'left', 'right']

    def reset(self):
        self.agent_position = self.start
        return self.agent_position

    def step(self, action):
        x, y = self.agent_position

        if action == 'up':
            x -= 1
        elif action == 'down':
            x += 1
        elif action == 'left':
            y -= 1
        elif action == 'right':
            y += 1

        # Check if the new position is within bounds and not a wall
        if 0 <= x < len(self.maze) and 0 <= y < len(self.maze[0]) and self.maze[x][y] != 1:
            self.agent_position = (x, y)

        # Check if the agent reached the goal
        if self.agent_position == self.goal:
            return self.agent_position, 10  # Goal reached (reward 10)

        return self.agent_position, -1  # Negative reward for each step (penalty)

class QLearningAgent:
    def __init__(self, actions, learning_rate=0.1, discount_factor=0.9, exploration_rate=1.0, exploration_decay=0.995):
        self.actions = actions
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.exploration_rate = exploration_rate
        self.exploration_decay = exploration_decay
        self.q_table = {}

    def get_state_key(self, state):
        return str(state)

    def choose_action(self, state):
        # Exploration vs exploitation
        if random.uniform(0, 1) < self.exploration_rate:
            return random.choice(self.actions)
        else:
            state_key = self.get_state_key(state)
            if state_key not in self.q_table:
                return random.choice(self.actions)
            return max(self.q_table[state_key], key=self.q_table[state_key].get)

    def update_q_table(self, state, action, reward, next_state):
        state_key = self.get_state_key(state)
        next_state_key = self.get_state_key(next_state)

        if state_key not in self.q_table:
            self.q_table[state_key] = {a: 0 for a in self.actions}

        if next_state_key not in self.q_table:
            self.q_table[next_state_key] = {a: 0 for a in self.actions}

        old_q_value = self.q_table[state_key][action]
        max_future_q = max(self.q_table[next_state_key].values())

        # Q-learning update rule
        new_q_value = old_q_value + self.learning_rate * (reward + self.discount_factor * max_future_q - old_q_value)
        self.q_table[state_key][action] = new_q_value

    def decay_exploration(self):
        self.exploration_rate *= self.exploration_decay

# Maze Setup (0 = empty, 1 = wall, 2 = goal)
maze = [
    [0, 0, 0, 1, 0],
    [0, 1, 0, 1, 0],
    [0, 0, 0, 0, 0],
    [1, 1, 1, 0, 1],
    [0, 0, 0, 0, 2]
]

start = (0, 0)
goal = (4, 4)

# Initialize environment and agent
env = MazeEnvironment(maze, start, goal)
agent = QLearningAgent(actions=['up', 'down', 'left', 'right'])

# Training Loop
for episode in range(1000):
    state = env.reset()
    total_reward = 0
    done = False

    while not done:
        action = agent.choose_action(state)
        next_state, reward = env.step(action)

        agent.update_q_table(state, action, reward, next_state)
        agent.decay_exploration()

        state = next_state
        total_reward += reward

        if state == goal:
            done = True

    if episode % 100 == 0:
        print(f"Episode {episode}: Total Reward = {total_reward}")

# After training, test the learned agent
state = env.reset()
steps = 0
while state != goal:
    action = agent.choose_action(state)
    state, reward = env.step(action)
    steps += 1

print(f"Goal reached in {steps} steps!")

import numpy as np

# Representing the maze as a 5x5 grid.
# 0 = empty space, 1 = wall, 2 = goal
maze = np.array([
    [0, 0, 0, 0, 0],
    [0, 1, 1, 1, 0],
    [0, 1, 0, 0, 0],
    [0, 1, 0, 1, 0],
    [0, 0, 0, 1, 2]
])

start = (0, 0)  # Starting position
goal = (4, 4)   # Goal position

class QLearningAgent:
    def __init__(self, actions, learning_rate=0.1, discount_factor=0.9, exploration_rate=1.0, exploration_decay=0.995):
        self.actions = actions
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.exploration_rate = exploration_rate
        self.exploration_decay = exploration_decay
        self.q_table = {}

    def get_state_key(self, state):
        return str(state)

    def choose_action(self, state):
        # Exploration vs Exploitation
        if random.uniform(0, 1) < self.exploration_rate:  # Exploration
            return random.choice(self.actions)
        else:  # Exploitation
            state_key = self.get_state_key(state)
            if state_key not in self.q_table:
                return random.choice(self.actions)
            return max(self.q_table[state_key], key=self.q_table[state_key].get)

    def update_q_table(self, state, action, reward, next_state):
        state_key = self.get_state_key(state)
        next_state_key = self.get_state_key(next_state)

        if state_key not in self.q_table:
            self.q_table[state_key] = {a: 0 for a in self.actions}

        if next_state_key not in self.q_table:
            self.q_table[next_state_key] = {a: 0 for a in self.actions}

        old_q_value = self.q_table[state_key][action]
        max_future_q = max(self.q_table[next_state_key].values())

        # Q-learning update rule
        new_q_value = old_q_value + self.learning_rate * (reward + self.discount_factor * max_future_q - old_q_value)
        self.q_table[state_key][action] = new_q_value

    def decay_exploration(self):
        self.exploration_rate *= self.exploration_decay

class MazeEnvironment:
    def __init__(self, maze, start, goal):
        self.maze = maze
        self.start = start
        self.goal = goal
        self.agent_position = start
        self.actions = ['up', 'down', 'left', 'right']

    def reset(self):
        self.agent_position = self.start
        return self.agent_position

    def step(self, action):
        x, y = self.agent_position
        if action == 'up': x -= 1
        elif action == 'down': x += 1
        elif action == 'left': y -= 1
        elif action == 'right': y += 1

        # Ensure position is within bounds and not hitting a wall
        if 0 <= x < len(self.maze) and 0 <= y < len(self.maze[0]) and self.maze[x][y] != 1:
            self.agent_position = (x, y)

        if self.agent_position == self.goal:
            return self.agent_position, 10  # Reached goal, reward = 10
        return self.agent_position, -1  # Each move results in a small negative reward

import random

# Initialize the environment and agent
env = MazeEnvironment(maze, start=(0, 0), goal=(4, 4))
agent = QLearningAgent(actions=['up', 'down', 'left', 'right'])

# Train for a set number of episodes
for episode in range(1000):
    state = env.reset()
    done = False
    total_reward = 0

    while not done:
        action = agent.choose_action(state)
        next_state, reward = env.step(action)

        agent.update_q_table(state, action, reward, next_state)
        agent.decay_exploration()  # Decay exploration rate

        state = next_state
        total_reward += reward

        if state == (4, 4):  # Goal reached
            done = True
    print(f"Episode {episode}: Total Reward = {total_reward}")

# Test the trained agent
state = env.reset()
steps = 0

while state != (4, 4):  # Goal position
    action = agent.choose_action(state)  # Agent chooses action based on learned Q-table
    state, reward = env.step(action)
    steps += 1

print(f"Goal reached in {steps} steps!")

# Test the trained agent for multiple episodes and calculate average steps
episodes_to_test = 100
total_steps = 0

for episode in range(episodes_to_test):
    state = env.reset()
    steps = 0
    done = False

    while state != (4, 4):  # Goal position
        action = agent.choose_action(state)  # Agent chooses action based on learned Q-table
        state, reward = env.step(action)
        steps += 1

    total_steps += steps

average_steps = total_steps / episodes_to_test
print(f"Average steps to reach the goal over {episodes_to_test} episodes: {average_steps}")

import matplotlib.pyplot as plt

def plot_maze_with_path(path):
    maze_copy = maze.copy()
    for position in path:
        maze_copy[position] = 3  # Mark the path with '3'
    plt.imshow(maze_copy, cmap='hot', interpolation='nearest')
    plt.show()

# Run a test to visualize the path
state = env.reset()
path = [state]
done = False

while state != (4, 4):  # Goal position
    action = agent.choose_action(state)
    state, reward = env.step(action)
    path.append(state)

plot_maze_with_path(path)

# Define the Q-learning parameters
alpha = 0.2  # Learning rate
gamma = 0.9  # Discount factor
epsilon = 0.2  # Exploration rate

epsilon = max(0.2, epsilon * 0.995)  # Decay exploration rate over time

import matplotlib.pyplot as plt

# Visualize the final path taken by the agent
def plot_maze_with_path(path, maze):
    maze_copy = maze.copy()
    for position in path:
        maze_copy[position] = 3  # Mark the path with '3'
    plt.imshow(maze_copy, cmap='hot', interpolation='nearest')
    plt.title("Agent's Path in the Maze")
    plt.show()

# After running the agent, plot the path
plot_maze_with_path(path, maze)

!pip install fpdf

from fpdf import FPDF

# Create PDF document
pdf = FPDF()
pdf.add_page()

pdf.set_font("Arial", size=12)
pdf.cell(200, 10, txt="Maze Navigation Agent: Reinforcement Learning Report", ln=True, align='C')
pdf.ln(10)

# Add sections
pdf.cell(200, 10, txt="Introduction", ln=True)
pdf.multi_cell(0, 10, txt="In this project, we designed a reinforcement learning agent to navigate a maze. "
                          "The agent learns to find the most efficient path from the start to the goal while avoiding obstacles.")
pdf.ln(5)

pdf.cell(200, 10, txt="Testing and Evaluation", ln=True)
pdf.multi_cell(0, 10, txt=f"The agent was able to complete the maze in an average of {average_steps} steps.")
pdf.ln(5)

# Save PDF
pdf.output("maze_navigation_report.pdf")

pip install stable-baselines3[extra]

!pip install stable-baselines3[extra]

import stable_baselines3
print(stable_baselines3.__version__)

import gym
import numpy as np
from stable_baselines3 import DQN

# Define a basic Gym environment
class MazeEnv(gym.Env):
    def __init__(self):
        # Define action & observation space
        self.action_space = gym.spaces.Discrete(4)  # Up, Down, Left, Right
        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(5,5), dtype=np.float32)

    def step(self, action):
        # Apply movement logic & return state, reward, done, info
        pass

    def reset(self):
        # Reset the environment
        pass

# Initialize environment
env = MazeEnv()

!pip install "shimmy>=2.0"

import gym
import numpy as np
from stable_baselines3 import DQN

class MazeEnv(gym.Env):
    def __init__(self):
        super(MazeEnv, self).__init__()

        # Define action space (Up, Down, Left, Right)
        self.action_space = gym.spaces.Discrete(4)

        # Define observation space (5x5 grid)
        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(5,5), dtype=np.float32)

        # Define initial and goal positions
        self.start_pos = (0, 0)
        self.goal_pos = (4, 4)
        self.agent_pos = self.start_pos

        # Initialize the grid
        self.grid = np.zeros((5,5), dtype=np.float32)
        self.grid[self.goal_pos] = 1  # Goal state

    def step(self, action):
        x, y = self.agent_pos

        # Define movement: 0=Up, 1=Down, 2=Left, 3=Right
        if action == 0 and x > 0:    # Up
            x -= 1
        elif action == 1 and x < 4:  # Down
            x += 1
        elif action == 2 and y > 0:  # Left
            y -= 1
        elif action == 3 and y < 4:  # Right
            y += 1

        self.agent_pos = (x, y)

        # Reward system
        if self.agent_pos == self.goal_pos:
            reward = 10  # Large reward for reaching the goal
            done = True
        else:
            reward = -0.1  # Small penalty to encourage faster solutions
            done = False

        # Update grid
        state = np.zeros((5,5), dtype=np.float32)
        state[self.agent_pos] = 1  # Mark agent position
        state[self.goal_pos] = 1   # Keep goal position

        return state, reward, done, {}  # Returning expected 4 values

    def reset(self):
        self.agent_pos = self.start_pos  # Reset agent position
        state = np.zeros((5,5), dtype=np.float32)
        state[self.agent_pos] = 1
        state[self.goal_pos] = 1
        return state  # Must return an observation

# Initialize and wrap the environment
env = MazeEnv()

# Train with DQN
model = DQN("MlpPolicy", env, verbose=1)
model.learn(total_timesteps=10000)

# Save the trained model
model.save("maze_solver")

import gym
import numpy as np
from stable_baselines3 import DQN

class MazeEnv(gym.Env):
    def __init__(self):
        super(MazeEnv, self).__init__()

        # Define action space (Up, Down, Left, Right)
        self.action_space = gym.spaces.Discrete(4)

        # Define observation space (5x5 grid)
        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(5,5), dtype=np.float32)

        # Define initial and goal positions
        self.start_pos = (0, 0)
        self.goal_pos = (4, 4)
        self.agent_pos = self.start_pos

        # Initialize the grid
        self.grid = np.zeros((5,5), dtype=np.float32)
        self.grid[self.goal_pos] = 1  # Goal state

    def step(self, action):
        x, y = self.agent_pos

        # Define movement: 0=Up, 1=Down, 2=Left, 3=Right
        if action == 0 and x > 0:    # Up
            x -= 1
        elif action == 1 and x < 4:  # Down
            x += 1
        elif action == 2 and y > 0:  # Left
            y -= 1
        elif action == 3 and y < 4:  # Right
            y += 1

        self.agent_pos = (x, y)

        # Reward system
        if self.agent_pos == self.goal_pos:
            reward = 10  # Large reward for reaching the goal
            done = True
        else:
            reward = -0.1  # Small penalty to encourage faster solutions
            done = False

        # Update grid
        state = np.zeros((5,5), dtype=np.float32)
        state[self.agent_pos] = 1  # Mark agent position
        state[self.goal_pos] = 1   # Keep goal position

        return state, reward, done, {}  # Returning expected 4 values

    def reset(self):
        self.agent_pos = self.start_pos  # Reset agent position
        state = np.zeros((5,5), dtype=np.float32)
        state[self.agent_pos] = 1
        state[self.goal_pos] = 1
        return state  # Must return an observation

    def render(self, mode="human"):
        """Print the maze with agent position"""
        grid_display = np.full((5,5), "⬜")  # Empty cells
        grid_display[self.goal_pos] = "🏁"  # Goal position
        grid_display[self.agent_pos] = "🤖"  # Agent position

        print("\n".join(["".join(row) for row in grid_display]))
        print("\n" + "-"*10)

# Initialize and wrap the environment
env = MazeEnv()

# Train with DQN
model = DQN("MlpPolicy", env, verbose=1)
model.learn(total_timesteps=10000)

# Save the trained model
model.save("maze_solver")

# Test rendering
obs = env.reset()
done = False

while not done:
    action, _states = model.predict(obs)
    obs, reward, done, info = env.step(action)
    env.render()  # Now properly implemented

obs = env.reset()
done = False
while not done:
    action, _states = model.predict(obs)
    obs, reward, done, info = env.step(action)
    env.render()

from fpdf import FPDF

pdf = FPDF()
pdf.add_page()
pdf.set_font("Arial", size=12)

pdf.cell(200, 10, "Machine Learning Report", ln=True, align='C')
pdf.multi_cell(0, 10, "Summary of Data Processing, Model Training, and Business Insights.")

pdf.output("report.pdf")